{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Copy of 188c_hw2.ipynb",
      "provenance": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mikofarin12/practice_assignment/blob/master/Copy_of_188c_hw2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eP5ESmvXPYWi"
      },
      "source": [
        "#188C Homework 2: Twitter Data Analysis\n",
        "In Homework 1, we collected Twitter data from 10 politicians(5 democrats and 5 republicans). In this homework, we will find the word useage frequency, which can reflect the focus of the two parties.\n",
        "\n",
        "Any changes won't be saved on this example notebook. Please edit on your own copy. \n",
        "\n",
        "Please submit the pdf version of your notebook(on the top left menu, click File --> Print to get the pdf)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mgdVa-W0QmeW"
      },
      "source": [
        "##Step 1: Load Data to notebook "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PF1eQb6sPSP1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c721c97c-7b5c-40ba-8b5c-0e523d7a1835"
      },
      "source": [
        "# Import needed libraries\n",
        "from google.colab import drive  # to mount Google Drive to Colab notebook\n",
        "import tweepy                   # Python wrapper around Twitter API\n",
        "import json\n",
        "import pandas as pd\n",
        "import csv\n",
        "from datetime import date\n",
        "from datetime import datetime\n",
        "import time\n",
        "import matplotlib.pyplot as plt\n",
        "import os\n",
        "\n",
        "#use the same code from hw1\n",
        "\n",
        "# Connect to Twitter API using the secrets\n",
        "auth = tweepy.OAuthHandler(\"aETIEZtuiP2gQV8wx2AxNLIEf\", \"CYywIpkSA6jLtoEjCG34If8weD4GxjoNBBi9aC4VumjDNzGSdc\")\n",
        "auth.set_access_token(\"1347299031593013248-2A0tKaIaM8x7sxiVt00Lc97MqKEmJZ\", \"PBmZnGZGAgVuS6rdeLcGcyU6tYITrcMpz2NUrQhBFfm6g\")\n",
        "api = tweepy.API(auth)\n",
        "\n",
        "def get_all_tweets(screen_name):\n",
        "  '''\n",
        "  collect the lastest 1000 tweets from one specific user \n",
        "  input: twitter user name(str)\n",
        "  '''\n",
        "\n",
        "\t# initialize a list to hold all the tweepy Tweets\n",
        "  alltweets = []\n",
        "\n",
        "\t# make initial request for most recent tweets (use 200 as the maximum allowed count)\n",
        "  recent_tweets = api.user_timeline(screen_name = screen_name,count=200)\n",
        "\n",
        "\t# save most recent tweets\n",
        "  alltweets.extend(recent_tweets)\n",
        "\t\n",
        "\t# save the id of the oldest tweet \n",
        "  most_recent = alltweets[-1].id\n",
        "\t\n",
        "\t# write a loop to keep grabbing tweets until there are no tweets \n",
        "  #left to grab or reach 1000 tweets\n",
        "  while len(recent_tweets) > 0:\n",
        "    \n",
        "    #collect tweets by 200 per, use max_id to set where to start and avoid duplicates\n",
        "    recent_tweets = api.user_timeline(screen_name = screen_name,count=200, \n",
        "                                      max_id=most_recent)\n",
        "    #save most recent tweets\n",
        "    alltweets.extend(recent_tweets)\n",
        "    \n",
        "    #iterate to the next oldest tweet\n",
        "    most_recent = alltweets[-1].id -1\n",
        "    \n",
        "    #if close to 1000, since we are grabbing tweets by 200 per request, \n",
        "    #stop the loop for a maximum of 1000\n",
        "    if len(alltweets) >= 950:\n",
        "      print(\"finished scraping\", len(alltweets) ,\"tweets from\", screen_name, \"account\")\n",
        "      break\n",
        "\n",
        "  alltweets = [tweet.text for tweet in alltweets]\n",
        "  return(alltweets)\n",
        "\n",
        "\n",
        "# load only user tweets from Demnocrat party and save in the tweetsD list\n",
        "tweetsD = get_all_tweets('JoeBiden')+ get_all_tweets('KamalaHarris') + get_all_tweets('BarackObama')+ get_all_tweets('GavinNewsom')+ get_all_tweets('NYGovCuomo')\n",
        "\n",
        "# load only user tweets from Republican party and save in the tweetsR list\n",
        "tweetsR = get_all_tweets('Mike_Pence')+ get_all_tweets('mikepompeo') + get_all_tweets('federalreserve')+ get_all_tweets('MittRomney')+ get_all_tweets('Schwarzenegger')\n",
        "\n",
        "\n",
        "#checking if tweets collected correctly\n",
        "tweetsD[:5]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "finished scraping 1000 tweets from BarackObama account\n",
            "finished scraping 1000 tweets from KamalaHarris account\n",
            "finished scraping 1000 tweets from BarackObama account\n",
            "finished scraping 1000 tweets from GavinNewsom account\n",
            "finished scraping 1000 tweets from NYGovCuomo account\n",
            "finished scraping 976 tweets from Mike_Pence account\n",
            "finished scraping 1000 tweets from federalreserve account\n",
            "finished scraping 999 tweets from MittRomney account\n",
            "finished scraping 999 tweets from Schwarzenegger account\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[\"In each of us, there's a story to be shared—a story that’s sacred and can bring us together. I had such a good time… https://t.co/dZ6zavY9Mr\",\n",
              " 'Even as we focus on fighting COVID-19, it’s important to recognize that there’s another pandemic raging right now—o… https://t.co/lBFmTp9nsT',\n",
              " 'Sending my prayers to @TigerWoods and his family tonight—here’s to a speedy recovery for the GOAT of golf. If we’ve… https://t.co/fBtAC10hcv',\n",
              " 'Last year, I sat down with my good friend Bruce @Springsteen for a long and meaningful conversation that touched on… https://t.co/m9hWwkrtDl',\n",
              " 'For decades, John Lewis not only gave all of himself to the cause of freedom and justice, but inspired generations… https://t.co/trY8SGE47c']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "46wFmzs7SNld"
      },
      "source": [
        "##Step 2: Clean data\n",
        "In the end, we want to calculate the word usage frequency. Before we do the actual word counting, we can see from the raw tweet data that the tweets include urls which are not supposed to be considered in the vocabulary. Therefore, first we need to remove urls from tweets.\n",
        "\n",
        "Please the function remove_url(txt) below to remove urls from raw tweets and save new tweets in tweetsD_nourl and tweetsR_nourl"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SrUTG39LU9iL",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2b34c199-39ae-40e0-a7c1-eba23e934f23"
      },
      "source": [
        "import re\n",
        "def remove_url(txt):\n",
        "    \"\"\"Replace URLs found in a text string with nothing \n",
        "    (i.e. it will remove the URL from the string).\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    txt : string\n",
        "        A text string that you want to parse and remove urls.\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    The same txt string with url's removed.\n",
        "    \"\"\"\n",
        "\n",
        "    return \" \".join(re.sub(\"([^0-9A-Za-z \\t])|(\\w+:\\/\\/\\S+)\", \"\", txt).split())\n",
        "\n",
        "#taking url off tweets for Democrats\n",
        "tweetsD_nourl= [remove_url(tweet) for tweet in tweetsD]\n",
        "\n",
        "#taking url off tweets for Republicans\n",
        "tweetsR_nourl =[remove_url(tweet) for tweet in tweetsR]\n",
        "\n",
        "#checking if function is doing its job\n",
        "tweetsD_nourl[:3]\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['In each of us theres a story to be shareda story thats sacred and can bring us together I had such a good time',\n",
              " 'Even as we focus on fighting COVID19 its important to recognize that theres another pandemic raging right nowo',\n",
              " 'Sending my prayers to TigerWoods and his family tonightheres to a speedy recovery for the GOAT of golf If weve']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YsoWEN9aWNg9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 37
        },
        "outputId": "d7224911-4383-4cad-d672-5d0c9f68f455"
      },
      "source": [
        "# example usage\n",
        "tweet = 'The weather is good. https://t.co/JLY1DntIlR'\n",
        "remove_url(tweet)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'The weather is good'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f8ej4NIqVS1r"
      },
      "source": [
        "Besides urls, we can see words can be in the form of upper case and lower case. For example, CAT and cat are actually the same word. Therefore, we want to transform all words into lower case to aviod repeat count.\n",
        "\n",
        "Please go through each word in each tweets and use word.lower() to transform words into lower case form."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ugMyazrTWgt_",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9535d204-8f89-41f0-8139-128137bcc373"
      },
      "source": [
        "# example usage\n",
        "tweet = 'The weather is good. https://t.co/JLY1DntIlR'\n",
        "tweet_nourl = remove_url(tweet)\n",
        "tweet_lower = [word.lower() for word in tweet_nourl.split()]\n",
        "print(tweet_lower)\n",
        "\n",
        "tweet_lowerD = [tweet.lower().split() for tweet in tweetsD_nourl]\n",
        "\n",
        "\n",
        "tweet_lowerR = [tweet.lower().split() for tweet in tweetsR_nourl]\n",
        "\n",
        "#check if it works\n",
        "tweet_lowerD[0]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['the', 'weather', 'is', 'good']\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['in',\n",
              " 'each',\n",
              " 'of',\n",
              " 'us',\n",
              " 'theres',\n",
              " 'a',\n",
              " 'story',\n",
              " 'to',\n",
              " 'be',\n",
              " 'shareda',\n",
              " 'story',\n",
              " 'thats',\n",
              " 'sacred',\n",
              " 'and',\n",
              " 'can',\n",
              " 'bring',\n",
              " 'us',\n",
              " 'together',\n",
              " 'i',\n",
              " 'had',\n",
              " 'such',\n",
              " 'a',\n",
              " 'good',\n",
              " 'time']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R06v6M1eYfjA"
      },
      "source": [
        "The last step before we do the actual counting is to remove stop words. There are words that do not add meaningful information to the text. These words referred to as “stop words” and include commonly appearing words such as who, what, you, etc.\n",
        "\n",
        "Please use the nltk library to remove stop words in tweets."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aUtn2AxnatPy",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5e11c698-ea1e-4f67-8c6d-25a070c2162b"
      },
      "source": [
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "from nltk.corpus import stopwords\n",
        "\n",
        "stop_words = set(stopwords.words('english'))\n",
        "\n",
        "# View a few words from the set\n",
        "list(stop_words)[0:10]\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['a', 'for', \"you're\", 'then', 'ain', 'doing', 't', 'myself', 'he', 'its']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QhTCCsEWa3jC",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3c0f5ab6-f221-4020-b1ea-9f4ac4a88a82"
      },
      "source": [
        "# example usage\n",
        "tweet = ['the', 'weather', 'is', 'good']\n",
        "tweet_nostop = [word for word in tweet if word not in stop_words]\n",
        "print(tweet_nostop)\n",
        "\n",
        "#testing code\n",
        "tweet = ['last','year','i','sat','down','with','my','good','friend',\n",
        " 'bruce','springsteen','for','a','long','and','meaningful','conversation',\n",
        " 'that','touched','on']\n",
        "\n",
        "tweet_nostop = [word for word in tweet if word not in stop_words]\n",
        "print(tweet_nostop)\n",
        "\n",
        "#cleaned tweets for Democrats\n",
        "tweetD_nostop= [[word for word in tweet_words if word not in stop_words]\n",
        " for tweet_words in tweet_lowerD]\n",
        "\n",
        "#checking if code worked\n",
        "tweetD_nostop[10]\n",
        "\n",
        "#cleaned tweets for Republicans\n",
        "tweetR_nostop= [[word for word in tweet_words if word not in stop_words]\n",
        " for tweet_words in tweet_lowerR]\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['weather', 'good']\n",
            "['last', 'year', 'sat', 'good', 'friend', 'bruce', 'springsteen', 'long', 'meaningful', 'conversation', 'touched']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bhrMQUu9YWU0"
      },
      "source": [
        "##Step 3: build vocabulary\n",
        "After we get tweet_nostop, we can build vocabulary and count frequency useage of these two parties.\n",
        "\n",
        "1. Please find the top 30 most used word lists of these two parties as TopD and TopR. \n",
        "\n",
        "You may find collections.Counter() useful."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pvh2Xf13eOCS",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a4c32d5b-9b2f-43b8-e3fd-1aae473174cc"
      },
      "source": [
        "# print TopD and TopR and their corresponding frequency \n",
        "#import these libraries to perform the count\n",
        "import itertools\n",
        "import collections\n",
        "\n",
        "#clean our vector and create a list of words\n",
        "\n",
        "#democreats count\n",
        "cleaned_tweetsD = list(itertools.chain(*tweetD_nostop))\n",
        "\n",
        "# use suggested word count \n",
        "TopD = collections.Counter(cleaned_tweetsD)\n",
        "TopD= TopD.most_common(30)\n",
        "\n",
        "#Republican count\n",
        "cleaned_tweetsR = list(itertools.chain(*tweetR_nostop))\n",
        "\n",
        "# use suggested word count \n",
        "TopR = collections.Counter(cleaned_tweetsR)\n",
        "TopR = TopR.most_common(30)\n",
        "\n",
        "\n",
        "print(TopD)\n",
        "print(TopR)\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[('rt', 1043), ('today', 396), ('joebiden', 391), ('new', 370), ('vote', 338), ('people', 268), ('us', 266), ('covid', 260), ('day', 252), ('one', 230), ('update', 227), ('make', 226), ('get', 225), ('covid19', 219), ('leaders', 187), ('president', 187), ('yesterday', 186), ('todays', 176), ('first', 174), ('country', 170), ('health', 169), ('time', 161), ('tests', 157), ('americans', 156), ('need', 153), ('reported', 152), ('im', 146), ('election', 145), ('amp', 144), ('live', 143)]\n",
            "[('rt', 907), ('president', 357), ('amp', 295), ('today', 290), ('us', 258), ('schwarzenegger', 244), ('thank', 212), ('great', 205), ('america', 195), ('realdonaldtrump', 188), ('federalreserve', 186), ('years', 177), ('feddata', 177), ('weekly', 175), ('federal', 167), ('see', 161), ('live', 155), ('day', 154), ('chair', 141), ('back', 140), ('get', 136), ('reserve', 133), ('vice', 122), ('new', 122), ('watch', 119), ('american', 118), ('balancesheet', 117), ('vote', 115), ('people', 114), ('mikepence', 109)]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8q2AmeKUeLwZ"
      },
      "source": [
        "2. After finding TopD and TopR, please find the intersection of TopD and TopR as the vocabulary. Use two arrays to represent the word frequency of the two parties using words in the vocabulary. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "svajfl52hQrX",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5ebb4352-5d7c-426b-ba8b-d062e2899bd2"
      },
      "source": [
        "# example array\n",
        "vocabulary = ['weather', 'good']\n",
        "freqD = [100, 30] # use weather 100 times and good 30 times\n",
        "freqR = [50, 100] # use weather 50 times and good 100 times\n",
        "\n",
        "\n",
        "#collapse the list of words to find intersection\n",
        "collapsed_listD = list(itertools.chain(*TopD))\n",
        "collapsed_listR = list(itertools.chain(*TopR))\n",
        "\n",
        "#use code from online to define a function for finding intersection\n",
        "def intersection(lst1, lst2): \n",
        "    lst3 = [value for value in lst1 if value in lst2] \n",
        "    return lst3 \n",
        "\n",
        "#checking the intersection of the top vocabulary\n",
        "print(intersection(collapsed_listD,collapsed_listR))\n",
        "intersection_vocab = (intersection(collapsed_listD,collapsed_listR))\n",
        "\n",
        "#just getting the intersection of words, not numbers\n",
        "intersection_vocab = intersection_vocab[0:9] + intersection_vocab[11:13]\n",
        "print(intersection_vocab)\n",
        "\n",
        "#now we take out the non intersection words from out list Democracts\n",
        "tweetD_nostop_intersection =[[word for word in tweet_words if word in intersection_vocab]\n",
        " for tweet_words in tweetD_nostop]\n",
        "\n",
        "#democreats count intersection\n",
        "cleaned_tweetsD_intersection = list(itertools.chain(*tweetD_nostop_intersection))\n",
        "intersection_words_D = collections.Counter(cleaned_tweetsD_intersection)\n",
        "\n",
        "print(intersection_words_D)\n",
        "\n",
        "#now we take out the non intersection words from out list Democracts\n",
        "tweetR_nostop_intersection =[[word for word in tweet_words if word in intersection_vocab]\n",
        " for tweet_words in tweetR_nostop]\n",
        "\n",
        "\n",
        "#Republican count intersection\n",
        "cleaned_tweetsR_intersection = list(itertools.chain(*tweetR_nostop_intersection))\n",
        "intersection_words_R = collections.Counter(cleaned_tweetsR_intersection)\n",
        "\n",
        "\n",
        "print(intersection_words_R)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['rt', 'today', 'new', 'vote', 'people', 'us', 'day', 'get', 'president', 186, 161, 'amp', 'live']\n",
            "['rt', 'today', 'new', 'vote', 'people', 'us', 'day', 'get', 'president', 'amp', 'live']\n",
            "Counter({'rt': 1043, 'today': 396, 'new': 370, 'vote': 338, 'people': 268, 'us': 266, 'day': 252, 'get': 225, 'president': 187, 'amp': 144, 'live': 143})\n",
            "Counter({'rt': 907, 'president': 357, 'amp': 295, 'today': 290, 'us': 258, 'live': 155, 'day': 154, 'get': 136, 'new': 122, 'vote': 115, 'people': 114})\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-_Ieo9_ci67u"
      },
      "source": [
        "3. Here we will use chi-square test to find out whether parties have significant word useage difference.\n",
        "\n",
        "Please compute the p value using chi-square test library https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.chisquare.html and use p<0.05 as the accepted significance level\n",
        "\n",
        "More about chi-square test can found here https://math.hws.edu/javamath/ryan/ChiSquare.html."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dJ3mJPj5jtd2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c2d341a7-7b63-472f-85e3-624b529aa299"
      },
      "source": [
        "#now we create two arrays for word frequncy\n",
        "#Democrat Counter({'rt': 1043, 'today': 396, 'new': 370, 'vote': 338, 'people': 268, 'us': 266, 'day': 252, 'get': 225, 'president': 187, 'amp': 144, 'live': 143})\n",
        "freqD_intersection  = [1043,396,370,338,268,266,252,225,187,144,143]\n",
        "\n",
        "#Republican Counter({'rt': 907,'today': 290 , 'new': 122, 'vote': 115','people': 114, 'us': 258, 'day': 154, 'get': 136, president': 357, 'amp': 295, 'live': 155, })\n",
        "freqR_intersection = [907,290,122,115,114,258,154,136,357,295,155]\n",
        "\n",
        "from scipy.stats import chisquare\n",
        "chisquare(freqD_intersection ,freqR_intersection)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Power_divergenceResult(statistic=1483.7569366314883, pvalue=0.0)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 70
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ePy7LsCaL9Rk"
      },
      "source": [
        "## Since, p-value is 0.0 meaning <0.05 we reject the null hypothesis that there's no difference between the means and conclude that a significant difference does exist. In, context we find the the languages between democrats and republicans on twitter differ."
      ]
    }
  ]
}